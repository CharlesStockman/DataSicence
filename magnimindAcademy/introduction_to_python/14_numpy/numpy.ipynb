{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1715a8c3",
   "metadata": {},
   "source": [
    "# Numpy Assignment\n",
    "\n",
    "1- Create three lists representing house features, each list containing ten values. The first one for the house's size in square meters, the second one for rooms and last for price. Then, create an array combining these lists.\n",
    "\n",
    "2- Transpose the array you have created, so that every line can represent features of one house.\n",
    "\n",
    "3- Display the shape of the array and explain what it means.\n",
    "For the following assignment, you are going to use Earthquakes dataset.\n",
    "\n",
    "4- Load the Earthquakes dataset. Export the dataset to an array as you covered in the previous lesson.\n",
    "\n",
    "5- Slice first 20 rows and column numbers 3, 5, 6, 7, 12. Then, assign the array you sliced to a variable.\n",
    "\n",
    "6- Display the row numbers where last values are equal to 4.5 or higher.\n",
    "\n",
    "7- Assign 1 to first row.\n",
    "\n",
    "8- Save the final state of the array to disk. You are going to use this in the next assignment.\n",
    "\n",
    "9- Load the array you saved in the previous lesson from the disk.\n",
    "\n",
    "10- Display the mean and the standard deviation for each column.\n",
    "\n",
    "11- Subtract 1, 25, 25, 10, 4 from columns in order. (Remember it can be dobe in one line of code.)\n",
    "\n",
    "12- Multiply each element by 2. (Remember it can be done in one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f87f2ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:46.319873Z",
     "start_time": "2023-02-28T16:57:46.316403Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c27bf8",
   "metadata": {},
   "source": [
    "## Create three lists representing house features, \n",
    "\n",
    "Each list containing ten values. The first one for the house's size in square meters, the second one for rooms and last for price. Then, create an array combining these lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2f50c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:46.329903Z",
     "start_time": "2023-02-28T16:57:46.322852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of square foot_list 10\n",
      "length of rooms_list 10\n",
      "length of price_list 10\n",
      "shape of data structure  (3, 10)\n",
      "Array =  [[  1100   1200   1300   1400   1500   1600   1700   1800   1900   2000]\n",
      " [     3      4      5      6      7      8      9     10     11     12]\n",
      " [ 20000  40000  60000  80000 100000 120000 140000 160000 180000 200000]]\n"
     ]
    }
   ],
   "source": [
    "square_foot_list = [1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000]\n",
    "rooms_list = [i for i in np.arange(3,13)]\n",
    "price_list = [20000 * i for i in np.arange(1,11)]\n",
    "raw_house_data = np.array(\\\n",
    "            [square_foot_list,rooms_list,price_list])\n",
    "\n",
    "print(\"length of square foot_list\", len(square_foot_list))\n",
    "print(\"length of rooms_list\", len(rooms_list))\n",
    "print(\"length of price_list\", len(price_list))\n",
    "print(\"shape of data structure \", raw_house_data.shape)\n",
    "print(\"Array = \", raw_house_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9647c",
   "metadata": {},
   "source": [
    "## Transpose the array you have created\n",
    "\n",
    "So that every line can represent features of one house.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c199bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:46.336656Z",
     "start_time": "2023-02-28T16:57:46.332204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape =  (10, 3)\n",
      "array tranposed is  [[  1100      3  20000]\n",
      " [  1200      4  40000]\n",
      " [  1300      5  60000]\n",
      " [  1400      6  80000]\n",
      " [  1500      7 100000]\n",
      " [  1600      8 120000]\n",
      " [  1700      9 140000]\n",
      " [  1800     10 160000]\n",
      " [  1900     11 180000]\n",
      " [  2000     12 200000]]\n"
     ]
    }
   ],
   "source": [
    "raw_house_data = raw_house_data.copy().T\n",
    "print(\"shape = \", raw_house_data.shape)\n",
    "print(\"array tranposed is \", raw_house_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1ede9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T15:38:14.308339Z",
     "start_time": "2023-02-19T15:38:14.305503Z"
    }
   },
   "source": [
    "## Display shape of array and explain what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb36dd2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:46.343812Z",
     "start_time": "2023-02-28T16:57:46.340218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of house data is  (10, 3) \n",
      "\n",
      "The Array has been changed into a matrix that we can split apart.\n",
      "It can be split into features ( size, root ) and dependent (price). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of house data is \", raw_house_data.shape, \"\\n\")\n",
    "explamation  = \"\"\"The Array has been changed into a matrix that we can split apart.\n",
    "It can be split into features ( size, root ) and dependent (price). \n",
    "\"\"\"\n",
    "print(explamation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf783ec",
   "metadata": {},
   "source": [
    "## Load the Earthquakes dataset. \n",
    "* Export the dataset to an array as you covered in the previous \n",
    "lesson.\n",
    "*Load the Earthquakes dataset. \n",
    "*Export the dataset to an array as you covered in the previous lesson.\n",
    "\n",
    "It was 100 time easy to use pandas and convert into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f82c5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.534273Z",
     "start_time": "2023-02-28T16:57:46.346004Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'earthquakes1970-2014.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pd_csv_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearthquakes1970-2014.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m np_csv_data \u001b[38;5;241m=\u001b[39m pd_csv_data\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m      4\u001b[0m np_csv_data\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'earthquakes1970-2014.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd_csv_data = pd.read_csv(\"earthquakes1970-2014.csv\")\n",
    "np_csv_data = pd_csv_data.to_numpy()\n",
    "np_csv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248409d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.537308Z",
     "start_time": "2023-02-28T16:57:47.537286Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_csv_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7b4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.539142Z",
     "start_time": "2023-02-28T16:57:47.539117Z"
    }
   },
   "outputs": [],
   "source": [
    "pd_csv_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6d2af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T16:23:17.569232Z",
     "start_time": "2023-02-19T16:23:17.564484Z"
    }
   },
   "source": [
    "## Slice \n",
    "first 20 rows and column numbers 3, 5, 6, 7, 12. Then, assign the array you sliced to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20253ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.540749Z",
     "start_time": "2023-02-28T16:57:47.540731Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Convert to 0 index since 12 is out of bounds: Shape \", \\\n",
    "      np_csv_data.shape)\n",
    "print(\"The shape of the original data is \", np_csv_data.shape);\n",
    "\n",
    "data = np_csv_data[:20,[2,4,5,6,11] ]\n",
    "\n",
    "print(\"The shape of the new data is \", data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2eae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T16:31:04.893180Z",
     "start_time": "2023-02-19T16:31:04.888015Z"
    }
   },
   "source": [
    "## Display the row numbers \n",
    "\n",
    "where last values are equal to 4.5 or higher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839c57e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.542274Z",
     "start_time": "2023-02-28T16:57:47.542227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Problem -- I do not understand the question.\n",
    "# I have tried several different ideas, but I am not able to get a \n",
    "# good answer.  My plan is to discuss during office hours or after \n",
    "# next week and move on to the next probleem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94649d",
   "metadata": {},
   "source": [
    "## Assign 1 to first row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4891e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.543963Z",
     "start_time": "2023-02-28T16:57:47.543946Z"
    }
   },
   "outputs": [],
   "source": [
    "data[0,:] = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19dcb79",
   "metadata": {},
   "source": [
    "## Save the final state of the array to disk. \n",
    "\n",
    "You are going to use this in the next assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218ff0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.545744Z",
     "start_time": "2023-02-28T16:57:47.545728Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tried to save them all to string and convert them in the next \n",
    "# assignment, but it failed.  Has to resort to pandas to get the \n",
    "# numpy with the correct datatype\n",
    "#np.savetxt('data.csv', data, delimiter=',',fmt=\"%s\")\n",
    "import pandas as pd\n",
    "dataframe_to_save = pd.DataFrame(data)\n",
    "dataframe_to_save.to_csv(\"data.csv\", index=False)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a694c",
   "metadata": {},
   "source": [
    "## Load the array you saved from the disk.\n",
    "\n",
    "<b>The values array will be loaded as a Strings and will the datatype\n",
    "    changed for each column</b>\n",
    "\n",
    "<b>I have tried several ways and could not create a ndArray with multiple datatypes like Pandas could.</b>\n",
    "\n",
    "#### Best attempt at create a datatype with multiple datatypes \n",
    "When column stack was called all columns were converted to string\n",
    "``` python\n",
    "test2 = array_from_file.transpose()\n",
    "a = np.copy(test2[0]).astype(float).reshape(-1,1)\n",
    "b = np.copy(test2[1]).astype(float).reshape(-1,1)\n",
    "c = np.copy(test2[2]).astype(str).reshape(-1,1)\n",
    "\n",
    "test3 = np.column_stack((a,b,c))\n",
    "print(\"first row = \", test3[0])\n",
    "print(\"Shape = \", test3.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6dfe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.547046Z",
     "start_time": "2023-02-28T16:57:47.547030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Could not use this \n",
    "# array_from_file = np.loadtxt('data.csv', delimiter=',', dtype=str)\n",
    "import pandas as pd\n",
    "raw_csv_data = pd.read_csv(\"data.csv\", index_col=False)\n",
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab468f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.548655Z",
     "start_time": "2023-02-28T16:57:47.548639Z"
    }
   },
   "outputs": [],
   "source": [
    "np_csv_data = raw_csv_data.to_numpy()\n",
    "np_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc232f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-19T23:12:26.906720Z",
     "start_time": "2023-02-19T23:12:26.903214Z"
    }
   },
   "source": [
    "## Display mean, standard deviation for each column.\n",
    "\n",
    "Note the last column contains nan so it not a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7ee610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.550268Z",
     "start_time": "2023-02-28T16:57:47.550251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing -- Remove third column since it is a String\n",
    "numeric_data = np.delete(np_csv_data, 2, axis=1)\n",
    "\n",
    "# Calculate the mean\n",
    "print(type(numeric_data.mean(axis=0)))\n",
    "print(\"Mean of columns: \", np.round(numeric_data.mean(axis=0),2))\n",
    "\n",
    "# Preprocessing -- Need to remomve the NaN std was failing\n",
    "numeric_data_without_nan=np.nan_to_num(numeric_data_without_nan, nan=0)\n",
    "\n",
    "# Calculate the Standard Deviation\n",
    "print(\"Standard Deviation of Columns \",numeric_data_without_nan.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c49c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T00:21:06.042603Z",
     "start_time": "2023-02-20T00:21:06.038722Z"
    }
   },
   "source": [
    "## Subtract 1, 25, 25, 10, 4 from columns in order. \n",
    "\n",
    "(Remember it can be done in one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51273949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.552103Z",
     "start_time": "2023-02-28T16:57:47.552088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing remove the third element (25) since the third is a\n",
    "# string and we cannot use the minus operator on a string\n",
    "# However, we will use the result from above\n",
    "\n",
    "numeric_data - [1,25,10,4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124278d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T00:30:05.571410Z",
     "start_time": "2023-02-20T00:30:05.566513Z"
    }
   },
   "source": [
    "## Multiply each element by 2. \n",
    "\n",
    "(Remember it can be done in one line of code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf1a986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T16:57:47.553938Z",
     "start_time": "2023-02-28T16:57:47.553921Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_data * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28d776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
