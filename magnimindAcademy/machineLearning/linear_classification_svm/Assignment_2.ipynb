{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10899b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.374051Z",
     "start_time": "2023-05-08T15:44:11.987211Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4c7c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T20:01:54.585411Z",
     "start_time": "2023-05-07T20:01:54.568716Z"
    }
   },
   "source": [
    "## Face Recognition (from Python Data Science Handbook by Jake VanderPlas)\n",
    "\n",
    "As an example of support vector machines in action, let's take a look \n",
    "at the facial recognition problem.  We will use the Labeled Faces in \n",
    "the Wild dataset, which consists of several thousand collated photos \n",
    "of various public figures.  A fetcher for the dataset is built into \n",
    "]Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d624a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.490406Z",
     "start_time": "2023-05-08T15:44:17.376889Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min_faces_per_person=70 is too restrictive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the raw dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m faces_raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_lfw_people\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_faces_per_person\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(\"Target Variable -- \",faces_raw_data.target_names)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(\"Shape -- \", faces_raw_data.images.shape)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(\"Shape of one image \", faces_raw_data.images[0].shape)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/datasets/_lfw.py:339\u001b[0m, in \u001b[0;36mfetch_lfw_people\u001b[0;34m(data_home, funneled, resize, min_faces_per_person, color, slice_, download_if_missing, return_X_y)\u001b[0m\n\u001b[1;32m    336\u001b[0m load_func \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mcache(_fetch_lfw_people)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# load and memoize the pairs as np arrays\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m faces, target, target_names \u001b[38;5;241m=\u001b[39m \u001b[43mload_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_folder_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_faces_per_person\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_faces_per_person\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m X \u001b[38;5;241m=\u001b[39m faces\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(faces), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    349\u001b[0m fdescr \u001b[38;5;241m=\u001b[39m load_descr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlfw.rst\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/memory.py:594\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/memory.py:537\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    534\u001b[0m         must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[0;32m--> 537\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/joblib/memory.py:779\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 779\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[1;32m    781\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    783\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/datasets/_lfw.py:214\u001b[0m, in \u001b[0;36m_fetch_lfw_people\u001b[0;34m(data_folder_path, slice_, color, resize, min_faces_per_person)\u001b[0m\n\u001b[1;32m    212\u001b[0m n_faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(file_paths)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_faces \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_faces_per_person=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is too restrictive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m min_faces_per_person\n\u001b[1;32m    216\u001b[0m     )\n\u001b[1;32m    218\u001b[0m target_names \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(person_names)\n\u001b[1;32m    219\u001b[0m target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msearchsorted(target_names, person_names)\n",
      "\u001b[0;31mValueError\u001b[0m: min_faces_per_person=70 is too restrictive"
     ]
    }
   ],
   "source": [
    "# Load the raw dataset\n",
    "faces_raw_data = fetch_lfw_people(min_faces_per_person=70)\n",
    "#print(\"Target Variable -- \",faces_raw_data.target_names)\n",
    "#print(\"Shape -- \", faces_raw_data.images.shape)\n",
    "#print(\"Shape of one image \", faces_raw_data.images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986fc6a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.496072Z",
     "start_time": "2023-05-08T15:44:17.492604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a face to see what we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fc0d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.634329Z",
     "start_time": "2023-05-08T15:44:17.501765Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(faces_raw_data.images[0], cmap='bone')\n",
    "ax.set(xticks=[], yticks=[])\n",
    "xlabel=faces_raw_data.target_names[faces_raw_data.target[0]]\n",
    "ax.set_xlabel(xlabel=xlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416db364",
   "metadata": {},
   "source": [
    "Each image contains [62×47] or nearly 3,000 pixels.\n",
    "We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal component analysis (we will learn about PCA later) to extract 150 fundamental components to feed into our support vector machine classifier.\n",
    "We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f1184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.642237Z",
     "start_time": "2023-05-08T15:44:17.637822Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2010f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-08T15:44:17.658203Z",
     "start_time": "2023-05-08T15:44:17.644372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Test/Training Set\n",
    "(X_train, X_test, y_train,  y_test ) = \\\n",
    "    train_test_split(faces_raw_data.data, faces_raw_data.target, \\\n",
    "                     random_state=42)\n",
    "print(\"Training Shapes: \", X_train.shape, y_train.shape)\n",
    "print(\"Testing Shapes: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfe176",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-08T15:44:11.993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "parameters = { \n",
    "    'svc__C':[1, 5, 10, 50],\n",
    "    'svc__gamma': [0.0001, 0.0005, .001, .005 ]}\n",
    "\n",
    "grid_1 = GridSearchCV(model, param_grid=parameters, verbose=5)\n",
    "grid_1.fit(X_train,y_train)\n",
    "print(\"Best Score is \", grid_1.best_score_)\n",
    "print(\"Best estimator is \", grid_1.best_estimator_)\n",
    "print(\"Best params is \", grid_1.best_params_)\n",
    "\n",
    "# support_vector_classification.C = 1\n",
    "# support_vector_classification.kernel = 'linear'\n",
    "# model = support_vector_classification.fit(X_train, y_train)\n",
    "# print(\"Training Score\", model.score(X_train,y_train))\n",
    "# print(\"Testing Score \", model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d387b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
